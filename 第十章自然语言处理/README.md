<!--
 * @Autor: xujiahuan
 * @Date: 2020-03-05 20:37:36
 * @LastEditors: xujiahuan
 * @LastEditTime: 2020-03-16 01:02:54
 -->
# 词嵌入(word embedding)
把词映射为实数域向量的技术也叫词嵌入


# one-hot缺点
One-hot无法准确表达不同词之间的相似度


# Word2vec
word2vec的提出是为了解决不同one-hot向量的余弦相似度为0的问题。
它将每个词表示成一个定长的向量,并使得这些向量能较好地表达不同词之间的相似和类比关系
word2vec包含了两个两个模型，跳字模型(skip-gram)和连续词袋模型(CBOW)
word2vec训练结束后，对于词典中的任一索引为i的词，外面均得到该词作为中心词和背景词的两组词向量


## Skip-gram
跳字模型假设基于某个词来生成它在文本序列周围的词
在跳字模型中，每个词被表示成两个d维向量，用来计算条件概率
当它为中心词时，表示为vi，为背景词时，表示为ui


## CBOW
连续词袋模型假设基于某中心词在文本序列前后的背景词来生成该中心词
同跳字模型不一样的点在于，外面一般使用连续词袋模型的背景词向量作为词的表征向量


## 二次采样
数据集中每个被索引词wi将有一定概率被丢弃，并且越高频的词被丢弃的概率越大

# 子词嵌入
fastText提出了子词嵌入方法，从而试图将构词信息引入word2vec中的跳字模型
每个中心词被表示成子词的集合
中心词的向量是所有子词向量的和
